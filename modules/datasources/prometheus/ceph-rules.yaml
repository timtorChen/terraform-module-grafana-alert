---
# yaml-language-server: $schema=../../rule-groups/schema.json
- name: metrics-check
  interval_seconds: 300
  rules:
    - name: ceph-no-data
      annotations:
        summary: "Ceph metrics return NoData"
        description: >
          The Ceph metrics has been no data for 5 minutes. 
          Please check if Ceph or Prometheus is working.
      condition: "Z"
      datas:
        - ref_id: "Z"
          model:
            expr: "absent(ceph_health_status) or vector(0)"
            instant: true
      no_data_state: "Alerting"
      exec_err_state: "Error"
      for: "5m"
      labels: &c
        severity: "critical"
- name: health
  interval_seconds: 300
  rules:
    - name: ceph-health-warning
      annotations:
        summary: "Ceph is in the WARNING state"
        description: >
          The cluster state has been HEALTH_WARN for more than 5 minutes.
          Please check 'ceph health detail' for more information.
      condition: "Z"
      datas:
        - ref_id: "A"
          model:
            expr: ceph_health_status{${ceph_selectors}}
        - ref_id: "B"
          model:
            type: reduce
            expression: "A"
            reducer: last
        - ref_id: "Z"
          model:
            type: math
            expression: "$B == 1"
      no_data_state: "NoData"
      exec_err_state: "Error"
      for: "5m"
      labels: &w
        severity: "warning"
    - name: ceph-health-error
      annotations:
        summary: "Ceph is in the ERROR state"
        description:
          The cluster state has been HEALTH_ERROR for more than 5 minutes.
          Please check 'ceph health detail' for more information.
      condition: "Z"
      datas:
        - ref_id: "A"
          model:
            expr: ceph_health_status{${ceph_selectors}}
        - ref_id: "B"
          model:
            type: reduce
            expression: "A"
            reducer: last
        - ref_id: "Z"
          model:
            type: math
            expression: "$B == 2"
      no_data_state: "NoData"
      exec_err_state: "Error"
      for: "5m"
      labels: *c
- name: mon
  interval_seconds: 30
  rules:
    - name: ceph-mon-quorum-at-risk
      annotations:
        summary: "Ceph monitor quorum is at risk"
        description: >
          Quorum requires a more than one-half of monitors to be active.
          Without quorum the cluster will become inoperable, affecting all services and connected clients.
        documentation: "https://docs.ceph.com/en/latest/rados/operations/health-checks#mon-down"
      condition: "Z"
      datas:
        - ref_id: "A"
          model:
            expr: |
              count(ceph_mon_quorum_status{${ceph_selectors}} == 1)
        - ref_id: "B"
          model:
            expr: |
              (floor(count(ceph_mon_metadata{${ceph_selectors}}) / 2) + 1)
        - ref_id: "C"
          model:
            type: reduce
            expression: "A"
            reducer: last
        - ref_id: "D"
          model:
            type: reduce
            expression: "B"
            reducer: last
        - ref_id: "Z"
          model:
            type: math
            expression: "$C <= $D"
      no_data_state: "NoData"
      exec_err_state: "Error"
      for: "30s"
      labels: *c
    - name: ceph-mon-down
      annotations:
        summary: "Ceph monitor is down"
        description: "One or more monitor(s) is down"
        documentation: "https://docs.ceph.com/en/latest/rados/operations/health-checks#mon-down"
      condition: "Z"
      datas:
        - ref_id: "A"
          model:
            expr: ceph_health_detail{name="MON_DOWN", ${ceph_selectors}}
        - ref_id: "B"
          model:
            type: reduce
            expression: "A"
            reducer: last
        - ref_id: "Z"
          model:
            type: math
            expression: "$B == 1"
      no_data_state: "NoData"
      exec_err_state: "Error"
      for: "30s"
      labels: *c
    - name: ceph-mon-disk-space-critical
      annotations:
        summary: "Filesystem space on at least one monitor is critically low"
        description: >
          The free space available to a monitor's store is critically low.
          You should increase the space available to the monitor(s).
        documentation: "https://docs.ceph.com/en/latest/rados/operations/health-checks#mon-disk-crit"
      condition: "Z"
      datas:
        - ref_id: "A"
          model:
            expr: ceph_health_detail{name="MON_DISK_CRIT", ${ceph_selectors}}
        - ref_id: "B"
          model:
            type: reduce
            expression: "A"
            reducer: last
        - ref_id: "Z"
          model:
            type: math
            expression: "$B == 1"
      no_data_state: "NoData"
      exec_err_state: "Error"
      for: "1m"
      labels: *c
    - name: ceph-mon-clock-skew
      annotations:
        summary: "Clock skew detected among monitors"
        description: >
          Ceph monitors rely on closely synchronized time to maintain quorum and cluster consistency.
          This event indicates that the time on at least one mon has drifted too far from the lead mon.
          Review cluster status with ceph -s. This will show which monitors are affected.
          Check the time sync status on each monitor host with 'ceph time-sync-status' and the state and peers of your ntpd or chrony daemon.
        documentation: "https://docs.ceph.com/en/latest/rados/operations/health-checks#mon-clock-skew"
      condition: "Z"
      datas:
        - ref_id: "A"
          model:
            expr: ceph_health_detail{name="MON_CLOCK_SKEW", ${ceph_selectors}}
        - ref_id: "B"
          model:
            type: reduce
            expression: "A"
            reducer: last
        - ref_id: "Z"
          model:
            type: math
            expression: "$B == 1"
      no_data_state: "NoData"
      exec_err_state: "Error"
      for: "1m"
      labels: *w
- name: osd
  interval_seconds: 300
  rules:
    - name: ceph-osd-down
      annotations:
        summary: "Ceph OSD(s) is down"
        description: "OSD {{ $labels.ceph_daemon }} has been down for 5 minutes."
        documentation: "https://docs.ceph.com/en/latest/rados/operations/health-checks#mon-down"
      condition: "Z"
      datas:
        - ref_id: "A"
          model:
            expr: |
              ceph_osd_up{${ceph_selectors}}
              * on(ceph_daemon) group_left(device_ids, instance) (ceph_disk_occupation{${ceph_selectors}})
        - ref_id: "B"
          model:
            type: reduce
            expression: "A"
            reducer: last
        - ref_id: "Z"
          model:
            type: math
            expression: "$B == 0"
      no_data_state: "NoData"
      exec_err_state: "Error"
      for: "5m"
      labels: *w
    - name: ceph-osd-near-full
      annotations:
        summary: "Ceph OSD(s) is nearly full"
        description: "OSD {{ $labels.ceph_daemon }} has exceeded 75% of its capacity."
      condition: "Z"
      datas:
        - ref_id: "A"
          model:
            expr: |
              ceph_osd_stat_bytes_used{${ceph_selectors}}/ceph_osd_stat_bytes{${ceph_selectors}}
              * on(ceph_daemon) group_left(device_ids, instance) (ceph_disk_occupation{${ceph_selectors}}) 
              * 100
        - ref_id: "B"
          model:
            type: reduce
            expression: "A"
            reducer: last
        - ref_id: "Z"
          model:
            type: math
            expression: "$B > 75"
      no_data_state: "NoData"
      exec_err_state: "Error"
      for: "5m"
      labels: *w
    - name: ceph-osd-backfill-full
      annotations:
        summary: "OSD is too full for backfill operations"
        description: >
          An OSD has reached the BACKFILL FULL threshold. This will prevent rebalance operations from completing. 
          Use 'ceph health detail' and 'ceph osd df' to identify the problem. 
          To resolve, add capacity to the affected OSD's failure domain, restore down/out OSDs, or delete unwanted data.
        documentation: "https://docs.ceph.com/en/latest/rados/operations/health-checks#osd-backfillfull"
      condition: "Z"
      datas:
        - ref_id: "A"
          model:
            expr: ceph_health_detail{name="OSD_BACKFILLFULL", ${ceph_selectors}}
        - ref_id: "B"
          model:
            type: reduce
            expression: "A"
            reducer: last
        - ref_id: "Z"
          model:
            type: math
            expression: "$B == 1"
      no_data_state: "NoData"
      exec_err_state: "Error"
      for: "1m"
      labels: *w
    - name: ceph-osd-too-many-repairs
      annotations:
        summary: "OSD reports a high number of read errors"
        description: "Reads from an OSD have used a secondary PG to return data to the client, indicating a potential failing drive."
        documentation: "https://docs.ceph.com/en/latest/rados/operations/health-checks#osd-too-many-repairs"
      condition: "Z"
      datas:
        - ref_id: "A"
          model:
            expr: ceph_health_detail{name="OSD_TOO_MANY_REPAIRS", ${ceph_selectors}}
        - ref_id: "B"
          model:
            type: reduce
            expression: "A"
            reducer: last
        - ref_id: "Z"
          model:
            type: math
            expression: "$B == 1"
      no_data_state: "NoData"
      exec_err_state: "Error"
      for: "1m"
      labels: *w
- name: mds
  interval_seconds: 60
  rules:
    - name: "ceph-mds-damaged"
      annotations:
        summary: "Ceph filesystem is damaged"
        description: >
          Filesystem metadata has been corrupted. Data may be inaccessible. 
          Analyze metrics from the MDS daemon admin socket, or escalate to support.
        documentation: "https://docs.ceph.com/en/latest/cephfs/health-messages#cephfs-health-messages"
      condition: "Z"
      datas:
        - ref_id: "A"
          model:
            expr: ceph_health_detail{name="MDS_DAMAGE", ${ceph_selectors}}
        - ref_id: "B"
          model:
            type: reduce
            expression: "A"
            reducer: last
        - ref_id: "Z"
          model:
            type: math
            expression: "$B == 1"
      no_data_state: "NoData"
      exec_err_state: "Error"
      for: "1m"
      labels: *c
    - name: "ceph-mds-offline"
      annotations:
        summary: "Ceph filesystem is offline"
        description: "All MDS ranks are unavailable. The MDS daemons managing metadata are down, rendering the filesystem offline."
        documentation: "https://docs.ceph.com/en/latest/cephfs/health-messages/#mds-all-down"
      condition: "Z"
      datas:
        - ref_id: "A"
          model:
            expr: ceph_health_detail{name="MDS_ALL_DOWN", ${ceph_selectors}}
        - ref_id: "B"
          model:
            type: reduce
            expression: "A"
            reducer: last
        - ref_id: "Z"
          model:
            type: math
            expression: "$B == 1"
      no_data_state: "NoData"
      exec_err_state: "Error"
      for: "1m"
      labels: *c
    - name: "ceph-mds-degraded"
      annotations:
        description: >
          One or more metadata daemons (MDS ranks) are failed or in a damaged state.
          At best the filesystem is partially available, at worst the filesystem is completely unusable.
        documentation: "https://docs.ceph.com/en/latest/cephfs/health-messages/#fs-degraded"
        summary: "CephFS filesystem is degraded"
      condition: "Z"
      datas:
        - ref_id: "A"
          model:
            expr: ceph_health_detail{name="FS_DEGRADED", ${ceph_selectors}}
        - ref_id: "B"
          model:
            type: reduce
            expression: "A"
            reducer: last
        - ref_id: "Z"
          model:
            type: math
            expression: "$B == 1"
      no_data_state: "NoData"
      exec_err_state: "Error"
      for: "1m"
      labels: *c
    - name: "ceph-mds-insufficient-standby"
      annotations:
        summary: "Ceph filesystem standby daemons too few"
        description: >
          The minimum number of standby daemons required by standby_count_wanted is less than the current number of standby daemons.
          Adjust the standby count or increase the number of MDS daemons.
        documentation: "https://docs.ceph.com/en/latest/cephfs/health-messages/#mds-insufficient-standby"
      condition: "Z"
      datas:
        - ref_id: "A"
          model:
            expr: ceph_health_detail{name="MDS_INSUFFICIENT_STANDBY", ${ceph_selectors}}
        - ref_id: "B"
          model:
            type: reduce
            expression: "A"
            reducer: last
        - ref_id: "Z"
          model:
            type: math
            expression: "$B == 1"
      no_data_state: "NoData"
      exec_err_state: "Error"
      for: 1m
      labels: *c
    - name: "ceph-mds-failure-no-standby"
      annotations:
        summary: "MDS daemon failed, no further standby available"
        description: >
          An MDS daemon has failed, leaving only one active rank and no available standby.
          Investigate the cause of the failure or add a standby MDS.
        documentation: "https://docs.ceph.com/en/latest/cephfs/health-messages/#fs-with-failed-mds"
      condition: "Z"
      datas:
        - ref_id: "A"
          model:
            expr: ceph_health_detail{name="FS_WITH_FAILED_MDS", ${ceph_selectors}}
        - ref_id: "B"
          model:
            type: reduce
            expression: "A"
            reducer: last
        - ref_id: "Z"
          model:
            type: math
            expression: "$B == 1"
      no_data_state: "NoData"
      exec_err_state: "Error"
      for: 1m
      labels: *c
    - name: "ceph-mds-readonly"
      annotations:
        summary: "Ceph filesystem in read only mode due to write error(s)"
        description: >
          The filesystem has switched to READ ONLY due to an unexpected error when writing to the metadata pool.
          Either analyze the output from the MDS daemon admin socket, or escalate to support.
        documentation: "https://docs.ceph.com/en/latest/cephfs/health-messages#cephfs-health-messages"
      condition: "Z"
      datas:
        - ref_id: "A"
          model:
            expr: ceph_health_detail{name="MDS_DAMAGE", ${ceph_selectors}}
        - ref_id: "B"
          model:
            type: reduce
            expression: "A"
            reducer: last
        - ref_id: "Z"
          model:
            type: math
            expression: "$B == 1"
      no_data_state: "NoData"
      exec_err_state: "Error"
      for: "1m"
      labels: *c
- name: mgr
  interval_seconds: 60
  rules:
    - name: "ceph-mgr-module-crash"
      annotations:
        summary: "A manager module has recently crashed"
        description: >
          One or more mgr modules have crashed and have yet to be acknowledged by an administrator.
          A crashed module may impact functionality within the cluster.
          Use the 'ceph crash' command to determine which module has failed, and archive it to acknowledge the failure.
        documentation: "https://docs.ceph.com/en/latest/rados/operations/health-checks#recent-mgr-module-crash"
      condition: "Z"
      datas:
        - ref_id: "A"
          model:
            expr: ceph_health_detail{name="RECENT_MGR_MODULE_CRASH", ${ceph_selectors}}
        - ref_id: "B"
          model:
            type: reduce
            expression: "A"
            reducer: last
        - ref_id: "Z"
          model:
            type: math
            expression: "$B == 1"
      no_data_state: "NoData"
      exec_err_state: "Error"
      for: "1m"
      labels: *c
- name: pg
  interval_seconds: 60
  rules:
    - name: "ceph-pg-inactive"
      annotations:
        summary: "Placement group(s) is inactive"
        description: >
          Placement group {{ $labels.name }} have been inactive for more than 5 minutes.
          Inactive placement group is not able to serve read/write requests.
      condition: "Z"
      datas:
        - ref_id: "A"
          model:
            expr: |
              (ceph_pg_total{${ceph_selectors}} - ceph_pg_active{${ceph_selectors}})
              * on(pool_id) group_left(name) ceph_pool_metadata{${ceph_selectors}}
        - ref_id: "B"
          model:
            type: reduce
            expression: "A"
            reducer: last
        - ref_id: "Z"
          model:
            type: math
            expression: "$B > 0"
      no_data_state: "NoData"
      exec_err_state: "Error"
      for: "5m"
      labels: *c
    - name: "ceph-pg-unclean"
      annotations:
        summary: "Placement group(s) is marked unclean"
        description: >
          Placement group {{ $labels.name }} have been unclean for more than 5 minutes.
          Unclean PGs have not recovered from a previous failure.
      condition: "Z"
      datas:
        - ref_id: "A"
          model:
            expr: |
              (ceph_pg_total{${ceph_selectors}} - ceph_pg_clean{${ceph_selectors}})
              * on(pool_id) group_left(name) ceph_pool_metadata{${ceph_selectors}}
        - ref_id: "B"
          model:
            type: reduce
            expression: "A"
            reducer: last
        - ref_id: "Z"
          model:
            type: math
            expression: "$B > 0"
      no_data_state: "NoData"
      exec_err_state: "Error"
      for: "5m"
      labels: *w
    - name: "ceph-pg-damaged"
      annotations:
        summary: "Placement group is damaged, manual intervention needed"
        description: >
          During data consistency checks (scrub), at least one PG has been flagged as being damaged or inconsistent.
          Check to see which PG is affected, and attempt a manual repair if necessary. 
          To list problematic placement groups, use 'rados list-inconsistent-pg <pool>'. 
          To repair PGs use the 'ceph pg repair <pg_num>' command.
        documentation: "https://docs.ceph.com/en/latest/rados/operations/health-checks#pg-damaged"
      condition: "A"
      datas:
        - ref_id: "A"
          model:
            expr: |
              ceph_health_detail{name=~"PG_DAMAGED|OSD_SCRUB_ERRORS", ${ceph_selectors}}
        - ref_id: "B"
          model:
            type: reduce
            expression: "A"
            reducer: last
        - ref_id: "Z"
          model:
            type: math
            expression: "$B == 1"
      no_data_state: "NoData"
      exec_err_state: "Error"
      for: "5m"
      labels: *c
    - name: "ceph-pg-recovery-at-risk"
      annotations:
        summary: "OSDs are too full for recovery"
        description: >
          Data redundancy is at risk since one or more OSDs are at or above the 'full' threshold. 
          Add more capacity to the cluster, restore down/out OSDs, or delete unwanted data.
        documentation: "https://docs.ceph.com/en/latest/rados/operations/health-checks#pg-recovery-full"
      condition: "Z"
      datas:
        - ref_id: "A"
          model:
            expr: |
              ceph_health_detail{name="PG_RECOVERY_FULL", ${ceph_selectors}}
        - ref_id: "B"
          model:
            type: reduce
            expression: "A"
            reducer: last
        - ref_id: "Z"
          model:
            type: math
            expression: "$B == 1"
      no_data_state: "NoData"
      exec_err_state: "Error"
      for: "5m"
      labels: *c
    - name: "ceph-pg-unavailable-blocking-io"
      annotations:
        summary: "PG is unavailable, blocking I/O"
        description: >
          Data availability is reduced, impacting the cluster's ability to service I/O. 
          One or more placement groups (PGs) are in a state that blocks I/O.
        documentation: "https://docs.ceph.com/en/latest/rados/operations/health-checks#pg-availability"
      condition: "Z"
      datas:
        - ref_id: "A"
          model:
            expr: |
              ceph_health_detail{name="PG_AVAILABILITY", ${ceph_selectors}}
        - ref_id: "B"
          model:
            type: reduce
            expression: "A"
            reducer: last
        - ref_id: "Z"
          model:
            type: math
            expression: "$B == 1"
      no_data_state: "NoData"
      exec_err_state: "Error"
      for: "5m"
      labels: *c
    - name: "ceph-pg-backfill-at-risk"
      annotations:
        summary: "Backfill operations are blocked due to lack of free space"
        description: >
          Data redundancy may be at risk due to lack of free space within the cluster.
          One or more OSDs have reached the 'backfillfull' threshold. Add more capacity, or delete unwanted data.
        documentation: "https://docs.ceph.com/en/latest/rados/operations/health-checks#pg-backfill-full"
      condition: "Z"
      datas:
        - ref_id: "A"
          model:
            expr: |
              ceph_health_detail{name="PG_BACKFILL_FULL", ${ceph_selectors}}
        - ref_id: "B"
          model:
            type: reduce
            expression: "A"
            reducer: last
        - ref_id: "Z"
          model:
            type: math
            expression: "$B == 1"
      no_data_state: "NoData"
      exec_err_state: "Error"
      for: "1m"
      labels: *c
    - name: "ceph-pg-high-per-ods"
      annotations:
        summary: "Placement groups per OSD is too high"
        description: >
          The number of placement groups per OSD is too high (exceeds the mon_max_pg_per_osd setting).
          Check that the pg_autoscaler has not been disabled for any pools with 'ceph osd pool autoscale-status', 
          and that the profile selected is appropriate. You may also adjust the target_size_ratio of a pool to guide the autoscaler 
          based on the expected relative size of the pool ('ceph osd pool set cephfs.cephfs.meta target_size_ratio .1') or set 
          the pg_autoscaler mode to 'warn' and adjust pg_num appropriately for one or more pools.
        documentation: "https://docs.ceph.com/en/latest/rados/operations/health-checks/#too-many-pgs"
      condition: "Z"
      datas:
        - ref_id: "A"
          model:
            expr: |
              ceph_health_detail{name="TOO_MANY_PGS", ${ceph_selectors}}
        - ref_id: "B"
          model:
            type: reduce
            expression: "A"
            reducer: last
        - ref_id: "Z"
          model:
            type: math
            expression: "$B == 1"
      no_data_state: "NoData"
      exec_err_state: "Error"
      for: "1m"
      labels: *w
    - name: "ceph-pg-not-scrubbed"
      annotations:
        summary: "Placement group(s) have not been scrubbed"
        description: >
          One or more PGs have not been scrubbed recently. Scrubs check metadata integrity, protecting against bit-rot. 
          They check that metadata is consistent across data replicas. When PGs miss their scrub interval, it may indicate 
          that the scrub window is too small, or PGs were not in a 'clean' state during the scrub window.
          You can manually initiate a scrub with: ceph pg scrub <pgid>.
        documentation: "https://docs.ceph.com/en/latest/rados/operations/health-checks#pg-not-scrubbed"
      condition: "Z"
      datas:
        - ref_id: "A"
          model:
            expr: |
              ceph_health_detail{name="PG_NOT_SCRUBBED", ${ceph_selectors}}
        - ref_id: "B"
          model:
            type: reduce
            expression: "A"
            reducer: last
        - ref_id: "Z"
          model:
            type: math
            expression: "$B == 1"
      no_data_state: "NoData"
      exec_err_state: "Error"
      for: "5m"
      labels: *w
    - name: "ceph-pg-not-deep-scrubbed"
      annotations:
        summary: "Placement group(s) have not been deep scrubbed"
        description: >
          One or more PGs have not been deep scrubbed recently. Deep scrubs protect against bit-rot.
          They compare data replicas to ensure consistency. When PGs miss their deep scrub interval, it may indicate
          that the window is too small or PGs were not in a 'clean' state during the deep-scrub window.
        documentation: "https://docs.ceph.com/en/latest/rados/operations/health-checks#pg-not-deep-scrubbed"
      condition: "Z"
      datas:
        - ref_id: "A"
          model:
            expr: |
              ceph_health_detail{name="PG_NOT_DEEP_SCRUBBED", ${ceph_selectors}}
        - ref_id: "B"
          model:
            type: reduce
            expression: "A"
            reducer: last
        - ref_id: "Z"
          model:
            type: math
            expression: "$B == 1"
      no_data_state: "NoData"
      exec_err_state: "Error"
      for: "5m"
      labels: *w
- name: pool
  interval_seconds: 60
  rules:
    - name: "ceph-pool-growth-warning"
      annotations:
        summary: "Pool growth rate may soon exceed capacity"
        description: "Pool '{{ $labels.name }}' will be full in less than 5 days assuming the average fill-up rate of the past 48 hours."
      condition: "Z"
      datas:
        - ref_id: "A"
          model:
            expr: |
              (predict_linear(ceph_pool_percent_used{${ceph_selectors}}[2d], 3600*24*5) 
              * on(pool_id) group_right() ceph_pool_metadata{${ceph_selectors}})
        - ref_id: "B"
          model:
            type: reduce
            expression: "A"
            reducer: last
        - ref_id: "Z"
          model:
            type: math
            expression: "$B >= 95"
      no_data_state: "NoData"
      exec_err_state: "Error"
      for: "5m"
      labels: *w
    - name: "ceph-pool-full"
      annotations:
        summary: "Pool is full - writes are blocked"
        description: >
          A pool has reached its MAX quota, or OSDs supporting the pool have reached the FULL threshold.
          Until this is resolved, writes to the pool will be blocked.
        documentation: "https://docs.ceph.com/en/latest/rados/operations/health-checks#pool-full"
      condition: "Z"
      datas:
        - ref_id: "A"
          model:
            expr: |
              ceph_health_detail{name="POOL_FULL", ${ceph_selectors}}
        - ref_id: "B"
          model:
            type: reduce
            expression: "A"
            reducer: last
        - ref_id: "Z"
          model:
            type: math
            expression: "$B == 1"
      no_data_state: "NoData"
      exec_err_state: "Error"
      for: "1m"
      labels: *c
    - name: "ceph-pool-near-full"
      annotations:
        summary: "One or more Ceph pools are nearly full"
        description: >
          A pool has exceeded the warning (percent full) threshold, or OSDs supporting the pool have reached the NEARFULL threshold.
          Writes may continue, but you are at risk of the pool going read-only if more capacity isn't made available.
          Determine the affected pool with 'ceph df detail', looking at QUOTA BYTES and STORED.
          Increase the pool's quota, or add capacity to the cluster first then increase the pool's quota (e.g. ceph osd pool set quota <pool_name> max_bytes <bytes>).
          Also ensure that the balancer is active.
        documentation: "https://docs.ceph.com/en/latest/rados/operations/health-checks/#pool-near-full"
      condition: "Z"
      datas:
        - ref_id: "A"
          model:
            expr: |
              ceph_health_detail{name="POOL_NEAR_FULL", ${ceph_selectors}}
        - ref_id: "B"
          model:
            type: reduce
            expression: "A"
            reducer: last
        - ref_id: "Z"
          model:
            type: math
            expression: "$B == 1"
      no_data_state: "NoData"
      exec_err_state: "Error"
      for: "5m"
      labels: *w
- name: rados
  interval_seconds: 30
  rules:
    - name: "ceph-object-missing"
      annotations:
        summary: "Object(s) marked UNFOUND"
        description: >
          The latest version of a RADOS object can not be found, even though all OSDs are up.
          I/O requests for this object from clients will block (hang).
          Resolving this issue may require the object to be rolled back to a prior version manually, and manually verified.
        documentation: "https://docs.ceph.com/en/latest/rados/operations/health-checks#object-unfound"
      condition: "Z"
      datas:
        - ref_id: "A"
          model:
            expr: |
              ceph_health_detail{name="OBJECT_UNFOUND", ${ceph_selectors}}
        - ref_id: "B"
          model:
            type: reduce
            expression: "A"
            reducer: last
        - ref_id: "Z"
          model:
            type: math
            expression: "$B == 1"
      no_data_state: "NoData"
      exec_err_state: "Error"
      for: "30s"
      labels: *c
- name: generic
  interval_seconds: 60
  rules:
    - name: "ceph-daemon-crash"
      annotations:
        summary: "One or more Ceph daemons have crashed, and are pending acknowledgement"
        description: >
          One or more daemons have crashed recently, and need to be acknowledged.
          This notification ensures that software crashes do not go unseen.
          To acknowledge a crash, use the 'ceph crash archive <id>' command.
        documentation: "https://docs.ceph.com/en/latest/rados/operations/health-checks/#recent-crash"
      condition: "Z"
      datas:
        - ref_id: "A"
          model:
            expr: |
              ceph_health_detail{name="RECENT_CRASH", ${ceph_selectors}}
        - ref_id: "B"
          model:
            type: reduce
            expression: "A"
            reducer: last
        - ref_id: "Z"
          model:
            type: math
            expression: "$B == 1"
      no_data_state: "NoData"
      exec_err_state: "Error"
      for: "1m"
      labels: *c
